name: Integration Tests

on:
  push:
    branches: [ main, develop, 'feature/*' ]
  pull_request:
    branches: [ main, develop ]
  workflow_dispatch:
    inputs:
      test_type:
        description: 'Type of tests to run'
        required: true
        default: 'all'
        type: choice
        options:
        - all
        - unit
        - integration
        - performance
        - benchmark

env:
  TESTING: true
  PYTHONPATH: ${{ github.workspace }}
  LOG_LEVEL: DEBUG

jobs:
  # Unit and Service Tests (Fast)
  unit-tests:
    name: Unit Tests
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        lfs: true  # Pull LFS files for audio samples
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt
    
    - name: Run unit tests
      run: |
        pytest tests/ -m "not integration and not slow and not benchmark" \
          --cov=app \
          --cov-report=xml \
          --cov-report=term \
          --cov-fail-under=80 \
          --maxfail=10
    
    - name: Upload coverage to Codecov
      uses: codecov/codecov-action@v3
      with:
        file: ./coverage.xml
        flags: unittests
        name: codecov-umbrella

  # Code Quality Checks
  code-quality:
    name: Code Quality
    runs-on: ubuntu-latest
    timeout-minutes: 5
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt
    
    - name: Run Black formatting check
      run: black --check app tests
    
    - name: Run isort import sorting check
      run: isort --check-only app tests
    
    - name: Run mypy type checking
      run: mypy app

  # Integration Tests (Medium duration)
  integration-tests:
    name: Integration Tests
    runs-on: ubuntu-latest
    timeout-minutes: 20
    needs: [unit-tests, code-quality]
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'integration' || github.event.inputs.test_type == ''
    
    strategy:
      matrix:
        test-group: [
          "voice_workflow",
          "audio_processing", 
          "ai_integration",
          "vault_integration"
        ]
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        lfs: true
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y ffmpeg
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt
    
    - name: Create test directories
      run: |
        mkdir -p /tmp/test_vault
        mkdir -p /tmp/test_uploads
        mkdir -p /tmp/test_sessions
    
    - name: Run integration tests
      run: |
        pytest tests/integration/test_${{ matrix.test-group }}.py \
          -m integration \
          --verbose \
          --tb=short \
          --maxfail=3
      env:
        OBSIDIAN_VAULT_PATH: /tmp/test_vault
        UPLOAD_DIR: /tmp/test_uploads
        SESSION_STORAGE_DIR: /tmp/test_sessions

  # Performance and Load Tests (Slow)
  performance-tests:
    name: Performance Tests
    runs-on: ubuntu-latest
    timeout-minutes: 30
    needs: [integration-tests]
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'performance' || github.event.inputs.test_type == 'benchmark'
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        lfs: true
    
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.11'
        cache: 'pip'
    
    - name: Install system dependencies
      run: |
        sudo apt-get update
        sudo apt-get install -y ffmpeg
    
    - name: Install Python dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements-dev.txt
    
    - name: Create test directories
      run: |
        mkdir -p /tmp/test_vault
        mkdir -p /tmp/test_uploads
        mkdir -p /tmp/test_sessions
    
    - name: Run performance benchmarks
      run: |
        pytest tests/performance/ \
          -m benchmark \
          --benchmark-only \
          --benchmark-json=benchmark_results.json \
          --verbose
      env:
        OBSIDIAN_VAULT_PATH: /tmp/test_vault
        UPLOAD_DIR: /tmp/test_uploads
        SESSION_STORAGE_DIR: /tmp/test_sessions
    
    - name: Upload benchmark results
      uses: actions/upload-artifact@v3
      with:
        name: benchmark-results
        path: benchmark_results.json

  # Docker Integration Tests
  docker-tests:
    name: Docker Integration
    runs-on: ubuntu-latest
    timeout-minutes: 25
    needs: [code-quality]
    if: github.event.inputs.test_type == 'all' || github.event.inputs.test_type == 'integration' || github.event.inputs.test_type == ''
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
      with:
        lfs: true
    
    - name: Set up Docker Buildx
      uses: docker/setup-buildx-action@v3
    
    - name: Build test Docker image
      run: |
        docker build -t dialtone-test .
    
    - name: Run Docker compose tests
      run: |
        cd tests/docker
        docker-compose -f test-environment.yml up -d
        
        # Wait for services to be healthy
        timeout 120 bash -c 'until docker-compose -f test-environment.yml ps | grep healthy; do sleep 5; done'
        
        # Run tests against Docker environment
        docker-compose -f test-environment.yml exec -T voice-notes-api-test \
          pytest tests/integration/ -m integration --maxfail=5
        
        # Cleanup
        docker-compose -f test-environment.yml down

  # Security and Dependency Scan
  security-scan:
    name: Security Scan
    runs-on: ubuntu-latest
    timeout-minutes: 10
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Run Trivy vulnerability scanner
      uses: aquasecurity/trivy-action@master
      with:
        scan-type: 'fs'
        scan-ref: '.'
        format: 'sarif'
        output: 'trivy-results.sarif'
    
    - name: Upload Trivy scan results to GitHub Security tab
      uses: github/codeql-action/upload-sarif@v2
      with:
        sarif_file: 'trivy-results.sarif'
    
    - name: Python dependency scan
      run: |
        python -m pip install --upgrade pip safety
        safety check --json --output safety-report.json || true
    
    - name: Upload security scan results
      uses: actions/upload-artifact@v3
      with:
        name: security-reports
        path: |
          trivy-results.sarif
          safety-report.json

  # Test Results Summary
  test-summary:
    name: Test Summary
    runs-on: ubuntu-latest
    needs: [unit-tests, integration-tests, performance-tests, docker-tests, security-scan]
    if: always()
    
    steps:
    - name: Checkout code
      uses: actions/checkout@v4
    
    - name: Download all artifacts
      uses: actions/download-artifact@v3
    
    - name: Generate test summary
      run: |
        echo "# Test Results Summary" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        echo "## Job Status" >> $GITHUB_STEP_SUMMARY
        echo "- Unit Tests: ${{ needs.unit-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Integration Tests: ${{ needs.integration-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Performance Tests: ${{ needs.performance-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Docker Tests: ${{ needs.docker-tests.result }}" >> $GITHUB_STEP_SUMMARY
        echo "- Security Scan: ${{ needs.security-scan.result }}" >> $GITHUB_STEP_SUMMARY
        echo "" >> $GITHUB_STEP_SUMMARY
        
        if [ -f benchmark-results/benchmark_results.json ]; then
          echo "## Performance Benchmarks" >> $GITHUB_STEP_SUMMARY
          echo "Benchmark results available in artifacts" >> $GITHUB_STEP_SUMMARY
        fi
        
        echo "## Coverage and Quality" >> $GITHUB_STEP_SUMMARY
        echo "Coverage reports and quality checks completed" >> $GITHUB_STEP_SUMMARY